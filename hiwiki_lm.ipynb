{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi2vec Language Model Pre-training\n",
    "\n",
    "The goal of this notebook is to train a language model using the [fast.ai](http://www.fast.ai/) version of [AWD LSTM Language Model](https://arxiv.org/abs/1708.02182), with data from [Hindi Wikipedia Dump](https://dumps.wikimedia.org/hiwiki/latest/hiwiki-latest-pages-articles.xml.bz2). \n",
    "\n",
    "## EDIT THIS \n",
    "# TK\n",
    "Using 40M/200k/200k tokens of train-validation-test split, we achieved validation perplexity of **27.81627 with 60,004 embeddings at 400 dimensions**, compared to state-of-the-art as of October 27, 2018 at **42.41 for English WikiText-2 by [Yang et al (2018)](https://arxiv.org/abs/1711.03953)**. To the best of our knowledge, there is no comparable research in Thai language at the point of writing (February 17, 2019).\n",
    "\n",
    "Our workflow is as follows:\n",
    "\n",
    "* Retrieve and process [Thai Wikipedia Dump](https://dumps.wikimedia.org/thwiki/latest/thwiki-latest-pages-articles.xml.bz2) according to [n-waves/ulmfit-multilingual](https://github.com/n-waves/ulmfit-multilingual)\n",
    "* Perform 40M/200k/200k tokens of train-validation-test split split\n",
    "* Minimal text cleaning and tokenization using `newmm` with frozen dictionary (`engine='ulmfit'`) of [pyThaiNLP](https://github.com/pyThaiNLP/pythainlp/)\n",
    "* Train language model\n",
    "* Evaluate model based on perplexity and eyeballing\n",
    "* Extract embeddings to use as \"word2vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T03:30:25.407566Z",
     "start_time": "2018-01-25T03:30:21.597641Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai import *    \n",
    "from fastai.text import * \n",
    "from fastai.callbacks import CSVLogger\n",
    "\n",
    "lang = \"hi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-03-07 16:41:01--  https://dumps.wikimedia.org/hiwiki/latest/hiwiki-latest-pages-articles.xml.bz2\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.155.106, 2620:0:861:4:208:80:155:106\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.155.106|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the Wikipedia dump, \n",
    "# Uncomment next 2 lines when running for the first time\n",
    "url = f\"https://dumps.wikimedia.org/{lang}wiki/latest/{lang}wiki-latest-pages-articles.xml.bz2\"\n",
    "!wget -c $url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bzip2: Output file hiwiki-latest-pages-articles.xml already exists.\r\n"
     ]
    }
   ],
   "source": [
    "# Extract the bz2 file\n",
    "filename = f\"{lang}wiki-latest-pages-articles.xml\"\n",
    "compressed_filename = filename+\".bz2\"\n",
    "# Uncomment when running for the first time\n",
    "!bzip2 -dk $compressed_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 'ulmfit-multilingual/.git/index'\n",
      "removed 'ulmfit-multilingual/.git/config'\n",
      "removed directory 'ulmfit-multilingual/.git/branches'\n",
      "removed 'ulmfit-multilingual/.git/HEAD'\n",
      "removed 'ulmfit-multilingual/.git/logs/HEAD'\n",
      "removed 'ulmfit-multilingual/.git/logs/refs/remotes/origin/HEAD'\n",
      "removed directory 'ulmfit-multilingual/.git/logs/refs/remotes/origin'\n",
      "removed directory 'ulmfit-multilingual/.git/logs/refs/remotes'\n",
      "removed 'ulmfit-multilingual/.git/logs/refs/heads/master'\n",
      "removed directory 'ulmfit-multilingual/.git/logs/refs/heads'\n",
      "removed directory 'ulmfit-multilingual/.git/logs/refs'\n",
      "removed directory 'ulmfit-multilingual/.git/logs'\n",
      "removed 'ulmfit-multilingual/.git/description'\n",
      "removed directory 'ulmfit-multilingual/.git/refs/tags'\n",
      "removed 'ulmfit-multilingual/.git/refs/remotes/origin/HEAD'\n",
      "removed directory 'ulmfit-multilingual/.git/refs/remotes/origin'\n",
      "removed directory 'ulmfit-multilingual/.git/refs/remotes'\n",
      "removed 'ulmfit-multilingual/.git/refs/heads/master'\n",
      "removed directory 'ulmfit-multilingual/.git/refs/heads'\n",
      "removed directory 'ulmfit-multilingual/.git/refs'\n",
      "removed 'ulmfit-multilingual/.git/objects/pack/pack-2193cb863374e22215d892bd36fafac0f468e0ff.pack'\n",
      "removed 'ulmfit-multilingual/.git/objects/pack/pack-2193cb863374e22215d892bd36fafac0f468e0ff.idx'\n",
      "removed directory 'ulmfit-multilingual/.git/objects/pack'\n",
      "removed directory 'ulmfit-multilingual/.git/objects/info'\n",
      "removed directory 'ulmfit-multilingual/.git/objects'\n",
      "removed 'ulmfit-multilingual/.git/hooks/commit-msg.sample'\n",
      "removed 'ulmfit-multilingual/.git/hooks/update.sample'\n",
      "removed 'ulmfit-multilingual/.git/hooks/applypatch-msg.sample'\n",
      "removed 'ulmfit-multilingual/.git/hooks/pre-commit.sample'\n",
      "removed 'ulmfit-multilingual/.git/hooks/post-update.sample'\n",
      "removed 'ulmfit-multilingual/.git/hooks/pre-rebase.sample'\n",
      "removed 'ulmfit-multilingual/.git/hooks/pre-push.sample'\n",
      "removed 'ulmfit-multilingual/.git/hooks/prepare-commit-msg.sample'\n",
      "removed 'ulmfit-multilingual/.git/hooks/pre-receive.sample'\n",
      "removed 'ulmfit-multilingual/.git/hooks/pre-applypatch.sample'\n",
      "removed directory 'ulmfit-multilingual/.git/hooks'\n",
      "removed 'ulmfit-multilingual/.git/info/exclude'\n",
      "removed directory 'ulmfit-multilingual/.git/info'\n",
      "removed 'ulmfit-multilingual/.git/packed-refs'\n",
      "removed directory 'ulmfit-multilingual/.git'\n",
      "removed 'ulmfit-multilingual/fastai_contrib/__init__.py'\n",
      "removed 'ulmfit-multilingual/fastai_contrib/learner.py'\n",
      "removed 'ulmfit-multilingual/fastai_contrib/data.py'\n",
      "removed 'ulmfit-multilingual/fastai_contrib/utils.py'\n",
      "removed 'ulmfit-multilingual/fastai_contrib/models.py'\n",
      "removed directory 'ulmfit-multilingual/fastai_contrib'\n",
      "removed 'ulmfit-multilingual/experiments/cls_test_wt103_1_vf.ipynb'\n",
      "removed 'ulmfit-multilingual/experiments/cls_test_wt103_1_f.ipynb'\n",
      "removed directory 'ulmfit-multilingual/experiments'\n",
      "removed 'ulmfit-multilingual/prepare_wiki-en.sh'\n",
      "removed 'ulmfit-multilingual/requirements.txt'\n",
      "removed 'ulmfit-multilingual/get_preprocessed_wikis.sh'\n",
      "removed 'ulmfit-multilingual/results/time_benchmark/logs.md'\n",
      "removed 'ulmfit-multilingual/results/time_benchmark/qrnn_benchmark.py'\n",
      "removed directory 'ulmfit-multilingual/results/time_benchmark'\n",
      "removed 'ulmfit-multilingual/results/MLDoc.md'\n",
      "removed 'ulmfit-multilingual/results/logs/es.md'\n",
      "removed 'ulmfit-multilingual/results/logs/qrnn-es.md'\n",
      "removed 'ulmfit-multilingual/results/logs/common.md'\n",
      "removed 'ulmfit-multilingual/results/logs/100examples.md'\n",
      "removed 'ulmfit-multilingual/results/logs/de.md'\n",
      "removed 'ulmfit-multilingual/results/logs/qrnn-ru.md'\n",
      "removed 'ulmfit-multilingual/results/logs/ru/bs_vs_lr.md'\n",
      "removed 'ulmfit-multilingual/results/logs/ru/nl8.md'\n",
      "removed 'ulmfit-multilingual/results/logs/ru/merity5.md'\n",
      "removed 'ulmfit-multilingual/results/logs/ru/lm-opti.md'\n",
      "removed 'ulmfit-multilingual/results/logs/ru/merity4.md'\n",
      "removed 'ulmfit-multilingual/results/logs/ru/5epochs.md'\n",
      "removed 'ulmfit-multilingual/results/logs/ru/wide2.md'\n",
      "removed directory 'ulmfit-multilingual/results/logs/ru'\n",
      "removed 'ulmfit-multilingual/results/logs/zeroshot.md'\n",
      "removed 'ulmfit-multilingual/results/logs/zh.md'\n",
      "removed 'ulmfit-multilingual/results/logs/ru.md'\n",
      "removed 'ulmfit-multilingual/results/logs/tokenization/ru.md'\n",
      "removed directory 'ulmfit-multilingual/results/logs/tokenization'\n",
      "removed 'ulmfit-multilingual/results/logs/cls/ulmfit on cls.md'\n",
      "removed 'ulmfit-multilingual/results/logs/cls/zeroshoot.md'\n",
      "removed directory 'ulmfit-multilingual/results/logs/cls'\n",
      "removed 'ulmfit-multilingual/results/logs/en.md'\n",
      "removed 'ulmfit-multilingual/results/logs/fastai.errors.txt'\n",
      "removed 'ulmfit-multilingual/results/logs/ja.md'\n",
      "removed 'ulmfit-multilingual/results/logs/qrnn-de.md'\n",
      "removed 'ulmfit-multilingual/results/logs/qrnn-zh.md'\n",
      "removed 'ulmfit-multilingual/results/logs/qrnn-en.md'\n",
      "removed 'ulmfit-multilingual/results/logs/it.md'\n",
      "removed 'ulmfit-multilingual/results/logs/qrnn-it.md'\n",
      "removed 'ulmfit-multilingual/results/logs/label_smoothing.md'\n",
      "removed 'ulmfit-multilingual/results/logs/fr.md'\n",
      "removed 'ulmfit-multilingual/results/logs/Timing.md'\n",
      "removed 'ulmfit-multilingual/results/logs/mldoc/noise/es10k-noise-lstm.md'\n",
      "removed 'ulmfit-multilingual/results/logs/mldoc/noise/es-random.md'\n",
      "removed 'ulmfit-multilingual/results/logs/mldoc/noise/es10k-noise.md'\n",
      "removed 'ulmfit-multilingual/results/logs/mldoc/noise/de10k-noise.md'\n",
      "removed 'ulmfit-multilingual/results/logs/mldoc/noise/random.md'\n",
      "removed directory 'ulmfit-multilingual/results/logs/mldoc/noise'\n",
      "removed 'ulmfit-multilingual/results/logs/mldoc/random-init.md'\n",
      "removed directory 'ulmfit-multilingual/results/logs/mldoc'\n",
      "removed 'ulmfit-multilingual/results/logs/Training qrnn on ML.txt'\n",
      "removed directory 'ulmfit-multilingual/results/logs'\n",
      "removed directory 'ulmfit-multilingual/results'\n",
      "removed 'ulmfit-multilingual/calc_100.sh'\n",
      "removed 'ulmfit-multilingual/ulmfit/test_model.model'\n",
      "removed 'ulmfit-multilingual/ulmfit/__init__.py'\n",
      "removed 'ulmfit-multilingual/ulmfit/__main__.py'\n",
      "removed 'ulmfit-multilingual/ulmfit/create_wikitext.py'\n",
      "removed 'ulmfit-multilingual/ulmfit/postprocess_wikitext.py'\n",
      "removed 'ulmfit-multilingual/ulmfit/README.md'\n",
      "removed 'ulmfit-multilingual/ulmfit/.ipynb_checkpoints/create_wikitext-checkpoint.py'\n",
      "removed directory 'ulmfit-multilingual/ulmfit/.ipynb_checkpoints'\n",
      "removed 'ulmfit-multilingual/ulmfit/pretrain_lm.py'\n",
      "removed 'ulmfit-multilingual/ulmfit/train_clas.py'\n",
      "removed directory 'ulmfit-multilingual/ulmfit'\n",
      "removed 'ulmfit-multilingual/prepare_xnli.sh'\n",
      "removed 'ulmfit-multilingual/MLDoc.md'\n",
      "removed 'ulmfit-multilingual/prepare_cls.py'\n",
      "removed 'ulmfit-multilingual/prepare_imdb.sh'\n",
      "removed 'ulmfit-multilingual/README.md'\n",
      "removed 'ulmfit-multilingual/prepare_wiki.sh'\n",
      "removed 'ulmfit-multilingual/.gitignore'\n",
      "removed 'ulmfit-multilingual/prepare_xnli.py'\n",
      "removed 'ulmfit-multilingual/tests/__init__.py'\n",
      "removed 'ulmfit-multilingual/tests/test_text_data.py'\n",
      "removed 'ulmfit-multilingual/tests/test_text_train.py'\n",
      "removed 'ulmfit-multilingual/tests/test_end_to_end.py'\n",
      "removed directory 'ulmfit-multilingual/tests'\n",
      "removed 'ulmfit-multilingual/prepare_mldoc.py'\n",
      "removed directory 'ulmfit-multilingual/'\n",
      "Cloning into 'ulmfit-multilingual'...\n",
      "remote: Enumerating objects: 121, done.\u001b[K\n",
      "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
      "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
      "remote: Total 1246 (delta 74), reused 75 (delta 42), pack-reused 1125\u001b[K\n",
      "Receiving objects: 100% (1246/1246), 494.96 KiB | 195.00 KiB/s, done.\n",
      "Resolving deltas: 100% (809/809), done.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment when running for the first time\n",
    "# !rm -rvf ulmfit-multilingual/\n",
    "!git clone https://github.com/n-waves/ulmfit-multilingual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the dataset creation, pre- and post-processing of [n-waves/ulmfit-multilingual](https://github.com/n-waves/ulmfit-multilingual):\n",
    "\n",
    "* `ulmfit/create_wikitext.py` - Download thwiki in json format and separate them into 40M/200k/200k tokens of train-validation-test split. Articles with least than 100 tokens are removed. Also perform tokenization with whitespaces as separators.\n",
    "* `ulmfit/postprocess_wikitext.py` - Replace numbers and replace out-of-vocabulary tokens with `xxunk` (frequency of less than 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'wikiextractor' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "# Uncomment when running for the first time\n",
    "!git clone https://github.com/attardi/wikiextractor/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat wikiextractor/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when running for the first time, this can take a few minutes\n",
    "# This is run in quiet mode, remove --quiet if you want to see progress status\n",
    "\n",
    "# !python wikiextractor/WikiExtractor.py $filename -o data/wiki_extr --json --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_00  wiki_12  wiki_24  wiki_36  wiki_48  wiki_60  wiki_72  wiki_84\twiki_96\n",
      "wiki_01  wiki_13  wiki_25  wiki_37  wiki_49  wiki_61  wiki_73  wiki_85\twiki_97\n",
      "wiki_02  wiki_14  wiki_26  wiki_38  wiki_50  wiki_62  wiki_74  wiki_86\twiki_98\n",
      "wiki_03  wiki_15  wiki_27  wiki_39  wiki_51  wiki_63  wiki_75  wiki_87\twiki_99\n",
      "wiki_04  wiki_16  wiki_28  wiki_40  wiki_52  wiki_64  wiki_76  wiki_88\n",
      "wiki_05  wiki_17  wiki_29  wiki_41  wiki_53  wiki_65  wiki_77  wiki_89\n",
      "wiki_06  wiki_18  wiki_30  wiki_42  wiki_54  wiki_66  wiki_78  wiki_90\n",
      "wiki_07  wiki_19  wiki_31  wiki_43  wiki_55  wiki_67  wiki_79  wiki_91\n",
      "wiki_08  wiki_20  wiki_32  wiki_44  wiki_56  wiki_68  wiki_80  wiki_92\n",
      "wiki_09  wiki_21  wiki_33  wiki_45  wiki_57  wiki_69  wiki_81  wiki_93\n",
      "wiki_10  wiki_22  wiki_34  wiki_46  wiki_58  wiki_70  wiki_82  wiki_94\n",
      "wiki_11  wiki_23  wiki_35  wiki_47  wiki_59  wiki_71  wiki_83  wiki_95\n"
     ]
    }
   ],
   "source": [
    "!ls data/wiki_extr/AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head data/wiki_extr/AA/wiki_00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "### Using StanfordNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install stanfordnlp\n",
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Languages is hi. Download models when running for the first time\n"
     ]
    }
   ],
   "source": [
    "print(f\"Selected Languages is {lang}. Download models when running for the first time\")\n",
    "# stanfordnlp.download(f\"{lang}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out StanfordNLP\n",
    "Uncomment below to try out StanfordNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'lang': f\"{lang}\", # Language code for the language to build the Pipeline in\n",
    "# }\n",
    "# nlp = stanfordnlp.Pipeline(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(\"२०१४ के चुनाव में भारतीय जनता पार्टी, जिसने भ्रस्टाचार और आर्थिक विकास के मुद्दे पर चुनाव लड़ा था, ने अभूतपूर्व बहुमत प्राप्त किया, और नरेंद्र मोदी को प्रधानमन्त्री नियुक्त किया गया।\")\n",
    "# print(*[f'text: {word.text+\" \"}\\tlemma: {word.lemma}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/jupyter/stanfordnlp_resources/hi_hdtb_models/hi_hdtb_tokenizer.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'processors': 'tokenize,mwt', # Comma-separated list of processors to use, we load tokenize and mwt only\n",
    "    'lang': f\"{lang}\", # Language code for the language to build the Pipeline in\n",
    "}\n",
    "nlp = stanfordnlp.Pipeline(**config)\n",
    "oc = nlp(\"२०१४ के चुनाव में भारतीय जनता पार्टी, जिसने भ्रस्टाचार और आर्थिक विकास के मुद्दे पर चुनाव लड़ा था, ने अभूतपूर्व बहुमत प्राप्त किया, और नरेंद्र मोदी को प्रधानमन्त्री नियुक्त किया गया।\")\n",
    "# print(*[f'text: {word.text+\" \"}\\tlemma: {word.lemma}\\tupos: {word.upos}\\txpos: {word.xpos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"२०१४ के चुनाव में भारतीय जनता पार्टी, जिसने भ्रस्टाचार और आर्थिक विकास के मुद्दे पर चुनाव लड़ा था, ने अभूतपूर्व बहुमत प्राप्त किया, और नरेंद्र मोदी को प्रधानमन्त्री नियुक्त किया गया।\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word.text for sent in doc.sentences for word in sent.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['२०१४',\n",
       " 'के',\n",
       " 'चुनाव',\n",
       " 'में',\n",
       " 'भारतीय',\n",
       " 'जनता',\n",
       " 'पार्टी',\n",
       " ',',\n",
       " 'जिसने',\n",
       " 'भ्रस्टाचार',\n",
       " 'और',\n",
       " 'आर्थिक',\n",
       " 'विकास',\n",
       " 'के',\n",
       " 'मुद्दे',\n",
       " 'पर',\n",
       " 'चुनाव',\n",
       " 'लड़ा',\n",
       " 'था',\n",
       " ',',\n",
       " 'ने',\n",
       " 'अभूतपूर्व',\n",
       " 'बहुमत',\n",
       " 'प्राप्त',\n",
       " 'किया',\n",
       " ',',\n",
       " 'और',\n",
       " 'नरेंद्र',\n",
       " 'मोदी',\n",
       " 'को',\n",
       " 'प्रधानमन्त्री',\n",
       " 'नियुक्त',\n",
       " 'किया',\n",
       " 'गया',\n",
       " '।']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rv data/wiki/ # remove previous old Moses Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp ulmfit-multilingual/ulmfit/create_wikitext.py create_wikitext_stanfordnlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/jupyter/stanfordnlp_resources/hi_hdtb_models/hi_hdtb_tokenizer.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "Writing to data/wiki/hi-2/hi.wiki.train.tokens...\n",
      "data/wiki/hi-2/hi.wiki.train.tokens. # documents: 13,210. # tokens: 2,002,437.\n",
      "Copying data/wiki/hi-2/hi.wiki.train.tokens to data/wiki/hi-100/hi.wiki.train.tokens & data/wiki/hi-all/hi.wiki.train.tokens.\n",
      "Writing to data/wiki/hi-2/hi.wiki.valid.tokens...\n",
      "data/wiki/hi-2/hi.wiki.valid.tokens. # documents: 1,724. # tokens: 200,137.\n",
      "Copying data/wiki/hi-2/hi.wiki.valid.tokens to data/wiki/hi-100/hi.wiki.valid.tokens & data/wiki/hi-all/hi.wiki.valid.tokens.\n",
      "Writing to data/wiki/hi-2/hi.wiki.test.tokens...\n",
      "data/wiki/hi-2/hi.wiki.test.tokens. # documents: 203. # tokens: 200,062.\n",
      "Copying data/wiki/hi-2/hi.wiki.test.tokens to data/wiki/hi-100/hi.wiki.test.tokens & data/wiki/hi-all/hi.wiki.test.tokens.\n",
      "Writing to data/wiki/hi-100/hi.wiki.train.tokens...\n",
      "Processed 20,000 documents. Total # tokens: 4,638,427.\n",
      "Processed 30,000 documents. Total # tokens: 7,672,846.\n",
      "Processed 60,000 documents. Total # tokens: 16,933,288.\n",
      "Processed 90,000 documents. Total # tokens: 23,385,163.\n",
      "data/wiki/hi-100/hi.wiki.train.tokens. # documents: 109,552. # tokens: 26,711,030.\n",
      "Writing to data/wiki/hi-all/hi.wiki.train.tokens...\n",
      "data/wiki/hi-all/hi.wiki.train.tokens. # documents: 0. # tokens: 0.\n"
     ]
    }
   ],
   "source": [
    "# !python create_wikitext_stanfordnlp.py -i data/wiki_extr/ -o data/wiki/ -l hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34336 data/wiki/hi-2/hi.wiki.train.tokens\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/wiki/hi-2/hi.wiki.train.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=मालिश=\r\n",
      "मालिश\r\n",
      "शरीर की बाहरी एवं नीचे स्थित मांशपेशियों एवं संयोजी उत्तकों को दबाना , हिला-डुलाना आदि मालिश ( Massage ) कहलाता है । इससे उनकी कार्य करने की क्षमतबढ़ती है और उनकी टूट-फूट का निवारन होता है । इससे आराम मिलता है और शरीर स्वस्थ रहता है ।\r\n",
      "मालिश मांसपेशियों और संयोजी ऊतक के सतही और गहरी परतों में हेरफेर समारोह बढ़ाने उपचार प्रक्रिया में सहायता और छूट और अच्छी तरह से किया जा रहा है को बढ़ावा देने के है। शब्द फ्रेंच मालिश सानना के घर्षण से आता है , या \" स्पर्श महसूस , या संभाल \" या लैटिन Massa से \" बड़े पैमाने पर , आटा \" अर्थ अरबी Massa अर्थ से cf . यूनानी क्रिया μάσσω ( massō ) \" संभाल करने के लिए , स्पर्श , हाथों से काम करने के लिए आटा गूंध. भेद anatripsis मालिश के लिए प्राचीन ग्रीक शब्द था और लैटिन frictio था ।\r\n",
      "मालिश पर अभिनय और दबाव के साथ शरीर से छेड़छाड़ शामिल है - संरचित , असंरचित , स्थिर , या चलती - तनाव , गति , या कंपन किया मैन्युअल रूप से या यांत्रिक एड्स के साथ. टारगेट ऊतकों मांसपेशियों , tendons , ligaments , प्रावरणी , त्वचा , जोड़ों , या अन्य संयोजी ऊतक , के रूप में के रूप में अच्छी तरह से लसीका वाहिकाओं , या गैस्ट्रोइंटेस्टाइनल प्रणाली के अंगों में शामिल हो सकते हैं । मालिश हाथ , उंगलियों , कोहनी , घुटनों , प्रकोष्ठ और पैर के साथ लागू किया जा सकता है । अस्सी विभिन्न मान्यता प्राप्त मालिश रूपरेखा खत्म हो गई हैं । सबसे मालिश चिकित्सा के रूप में शुरू करने के लिए कारणों से उद्धृत किया गया है ग्राहक की मांग और नैदानिक प्रभाव माना जाता है ।\r\n",
      "पेशेवर सेटिंग में मालिश इलाज किया जा रहा है जबकि एक मालिश की मेज पर झूठ बोल रही एक मालिश कुर्सी में बैठे , या फर्श पर एक चटाई पर झूठ बोल ग्राहक शामिल है । मालिश विषय को पूरी तरह या आंशिक रूप से unclothed हो सकता है । शरीर के हिस्सों तौलिए या शीट के साथ कवर किया जा सकता है । उन है जो एक कैरियर के रूप में अभ्यास मालिश masseurs , masseuses , या , अगर किसी भी तरह से प्रमाणित मालिश चिकित्सक के रूप में के रूप में संदर्भित कर रहे हैं ।\r\n",
      "मालिश पर लेखन रोम , ग्रीस , भारत , जापान , चीन , मिस्र और मेसोपोटामिया सहित कई प्राचीन सभ्यताओं में पाया गया है । एक संभव बाइबिल संदर्भ c.493 ई.पू. दस्तावेजों लोहबान के ज़ैक्सीस की पत्नियों की सुंदरता आहार ( एस्तेर , 2:12 ) के एक भाग के के रूप में तेल के साथ दैनिक \" उपचार \" से हिप्पोक्रेट्स 460 कि \" चिकित्सक ई.पू. में लिखा है कई बातों में अनुभव होना चाहिए , लेकिन मलाई में विश्वासपूर्वक \"\r\n",
      "प्राचीन चीनी पीला सम्राट \" त्वचा और मांस की मालिश \" की सिफारिश द्वारा हुआंग्डी Neijing किताब बुलाया । मालिश गर्भपात की तकनीक , गर्भवती पेट के लिए दबाव के आवेदन शामिल है , सदियों के लिए किया गया है दक्षिण पूर्व एशिया में प्रचलित है । कंबोडिया में अंगकोर वाट के एक बस राहतें की सजा मंदिर , 1150 लगभग दिनांक , एक औरत जो अंडरवर्ल्ड के लिए भेजा गया है पर इस तरह के एक गर्भपात प्रदर्शन दानव दर्शाया गया है । यह गर्भपात का सबसे पुराना ज्ञात दृश्य प्रतिनिधित्व माना जा रहा है ।\r\n",
      "रोमानिया में कुछ बीमारियों एक मालिश है जिसमें ग्राहक एक वश में भालू ने पर दलित द्वारा इलाज किया गया । मैराथन धावकों 2004 आईएनजी ताइपे इंटरनेशनल मैराथन में मालिश प्राप्त किया ।\r\n",
      "चीन : आधुनिक समय में , चीन में मालिश पारंपरिक ढांचे में पश्चिमी विचारों को अवशोषित द्वारा विकसित किया गया है । यह व्यापक रूप से अभ्यास और अस्पताल और मेडिकल स्कूल में पढ़ाया जाता है और प्राथमिक स्वास्थ्य का एक अनिवार्य हिस्सा है । [13]\r\n"
     ]
    }
   ],
   "source": [
    "!head data/wiki/hi-2/hi.wiki.train.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"data/wiki/{lang}-2/\"\n",
    "model_path = f\"models/wiki/{lang}-2\"\n",
    "assert Path(data_path).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def istitle(line):\n",
    "    return len(re.findall(r'^ ?=\\|[^=]*\\|= ?$', line)) != 0\n",
    "\n",
    "import gc\n",
    "def read_wiki_articles(filename):\n",
    "    articles = []\n",
    "    with open(filename, encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "    current_article = []\n",
    "    for i,line in enumerate(lines):\n",
    "        current_article.append(line)\n",
    "        if i < len(lines)-2 and lines[i+1].strip() == \"\" and istitle(lines[i+2]):\n",
    "            articles.append(\"\".join(current_article))\n",
    "            current_article = []\n",
    "        articles.append(\"\".join(current_article))\n",
    "        gc.collect()\n",
    "#     print(current_article[:10])\n",
    "    gc.collect()\n",
    "    print(f\"Wiki text was split to {len(articles)} articles\")\n",
    "    return pd.DataFrame({'texts': np.array(articles, dtype=np.object)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/wiki/hi-2/hi.wiki.valid.tokens\n",
      "=श चिंग=\n",
      "श चिंग\n",
      "श चिंग चीन का पहला काव्य ग्रंथ है जो ईसा पूर्व सातवीं सदी में रचा गया । ' श चिंह ' का अर्थ ह - ' सर्वश्रेष्ठ प्राचीन कविताओं का संग्रह ' । इस ग्रंथ में व्यंग्यात्मक कविता , प्रेम के गीत , वीर गाथा , श्रम के गीत तथा पूजा के गीत इत्यादि शामिल हैं । इस ग्रंथ का जन्म प्राचीन यूनानी होमर के महाकाव्य से भी कई सौ वर्ष से पहले हुआ था ।\n",
      "इस कविता ग्रंथ में ईसा पूर्व 11वीं शताब्दी से ले कर सातवीं शताब्दी तक के पांच सौ सालों की 305 कविताएं संगृहित हैं जो ग्रंथ तीन भागों में बंटे हुए हैं । इस के पहले भाग में उस जमाने के 15 राज्यों में प्रचलित 160 लोकगीत हैं , दूसरे भाग में पश्चिमी चो राजवंश के दरबारी अनुष्ठानों और कुलीन वर्ग की रस्म सभाओं में प्रयुक्त 105 गीतिकाव्य हैं , जबकि तीसरे भाग में श्रद्धा व पूजा से जुड़ी 40 कविताएं हैं जो मुख्य तौर पर पूर्वजों व देवताओं की महानता का गुणगान करती हैं ।\n",
      "=महाद्वीपीय विस्थापन=\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial_filename = f\"{data_path}{lang}.wiki.valid.tokens\"\n",
    "print(trial_filename)\n",
    "with Path(trial_filename).open(\"r\") as f:\n",
    "    content = f.readlines()[:5]\n",
    "    current_article = []\n",
    "    for line in content:\n",
    "        current_article.append(line)\n",
    "    print(\"\".join(current_article))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_df = read_wiki_articles(f'{data_path}{lang}.wiki.valid.tokens').iloc[:-1,:]\n",
    "# test_df = read_wiki_articles(f'{data_path}{lang}.wiki.test.tokens').iloc[:-1,:]\n",
    "# train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #join tokens together, I think this is unique to Thai tokenization\n",
    "# train_df['texts'] = train_df.texts.map(lambda x: ''.join(x.split('|')))\n",
    "# valid_df['texts'] = valid_df.texts.map(lambda x: ''.join(x.split('|')))\n",
    "# test_df['texts'] = test_df.texts.map(lambda x: ''.join(x.split('|')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fucking Nightmare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p \"/home/jupyter/hindi2vec/data/valid/\"\n",
    "# !cp data/wiki/hi-2/hi.wiki.valid.tokens /home/jupyter/hindi2vec/data/valid/hi.wiki.valid.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p \"/home/jupyter/hindi2vec/data/train/\"\n",
    "# !cp data/wiki/hi-2/hi.wiki.train.tokens /home/jupyter/hindi2vec/data/train/hi.wiki.train.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We used the `newmm` engine of `pyThaiNLP` to perform tokenization. Out of randomnum tokens from all of training set, we chose 60,000 embeddings (plus two for unknown and padding) of tokens which appeared more than twice (not typos) in the training set.\n",
    "\n",
    "\n",
    "We perform the following text processing:\n",
    "\n",
    "* Fix html tags to plain texts\n",
    "* Lowercase all English words and if a word is written in all caps, we put it in a lower case and add `xxup` before\n",
    "* Repetitive characters: Thai usually emphasizes adjectives by repeating the last character such as `อร่อยมากกกกกกก` to `อร่อยมาก xxrep 7 ` so that the word still retains its original form. \n",
    "* Normalize character order: for instance `นำ้` to `น้ำ`\n",
    "* Add spaces around / and #\n",
    "* Remove multiple spaces and newlines\n",
    "* Remove empty brackets of all types (`([{`) which might result from cleaning up\n",
    "* `pyThaiNLP`'s `newmm` word tokenizer with frozen dictionary (`engine ='ulmfit'`)  is used to tokenize the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hindi Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `newmm` tokenizer with a dictionary frozen as of 2018-10-23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T15:50:02.035245Z",
     "start_time": "2018-01-24T15:50:01.940191Z"
    }
   },
   "outputs": [],
   "source": [
    "# text='วิทยาศาสตร์ดาวเคราะห์เป็นสาขาวิชาที่ศึกษาเกี่ยวกับองค์ประกอบของดาวเคราะห์'\n",
    "# a = word_tokenizeimport stanfordnlp\n",
    "class HindiTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lang:str):\n",
    "        self.lang = lang\n",
    "        self.config = {\n",
    "            'processors': 'tokenize,mwt', # Comma-separated list of processors to use, we load tokenize and mwt only\n",
    "            'lang': f\"{self.lang}\", # Language code for the language to build the Pipeline in\n",
    "        }\n",
    "        self.nlp = stanfordnlp.Pipeline(**config)\n",
    "    \n",
    "    def setup(self, lang:str):\n",
    "        stanfordnlp.download(f\"{lang}\")\n",
    "        \n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        doc = self.nlp(t)\n",
    "        return [word.text for sent in doc.sentences for word in sent.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text, lang=\"hi\"):\n",
    "    config = {\n",
    "            'processors': 'tokenize,mwt', # Comma-separated list of processors to use, we load tokenize and mwt only\n",
    "            'lang': f\"{lang}\", # Language code for the language to build the Pipeline in\n",
    "    }\n",
    "    nlp = stanfordnlp.Pipeline(**config)\n",
    "    doc = nlp(text)\n",
    "    return [word.text for sent in doc.sentences for word in sent.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tok_func=HindiTokenizer, lang='hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk', 'xxpad', 'xxbos', 'xxfld', 'xxmaj', 'xxup', 'xxrep', 'xxwrep']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #integrated into pythainlp.ulmfit.utils\n",
    "from fastai.text.transform import *\n",
    "# from pythainlp.tokenize import word_tokenize\n",
    "# from pythainlp.util import normalize as normalize_char_order\n",
    "\n",
    "# class ThaiTokenizer(BaseTokenizer):\n",
    "#     \"Wrapper around a newmm tokenizer to make it a `BaseTokenizer`.\"\n",
    "#     def __init__(self, lang = 'th'):\n",
    "#         self.lang = lang\n",
    "#     def tokenizer(self, t):\n",
    "#         return word_tokenize(t,engine='ulmfit')\n",
    "#     def add_special_cases(self, toks):\n",
    "#         pass\n",
    "    \n",
    "def replace_rep_after(t):\n",
    "    \"Replace repetitions at the character level in `t` after the repetition\"\n",
    "    def _replace_rep(m):\n",
    "        c,cc = m.groups()\n",
    "        return f'{c} {TK_REP} {len(cc)+1} '\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return re_rep.sub(_replace_rep, t)\n",
    "\n",
    "def rm_useless_newlines(t):\n",
    "    \"Remove multiple newlines in `t`.\"\n",
    "    return re.sub('[\\n]{2,}', ' ', t)\n",
    "\n",
    "def rm_brackets(t):\n",
    "    \"Remove all empty brackets from `t`.\"\n",
    "    new_line = re.sub('\\(\\)','',t)\n",
    "    new_line = re.sub('\\{\\}','',new_line)\n",
    "    new_line = re.sub('\\[\\]','',new_line)\n",
    "    return(new_line)\n",
    "\n",
    "#in case we want to add more specific rules for thai\n",
    "pre_rules_th = [fix_html, replace_rep_after, normalize_char_order, \n",
    "                spec_add_spaces, rm_useless_spaces, rm_useless_newlines, rm_brackets]\n",
    "post_rules_th = [replace_all_caps, deal_caps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Bunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the language model based on 40M/200k/200k tokens of train-validation-test split from [Thai Wikipedia Dump](https://dumps.wikimedia.org/thwiki/latest/thwiki-latest-pages-articles.xml.bz2). Tokens are generated and numericalized filtering all words with frequency more than 3 and at maximum vocab size of 60,000 (plus unknown and padding tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = Tokenizer(tok_func = HindiTokenizer, lang = f\"{lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk', 'xxpad', 'xxbos', 'xxfld', 'xxmaj', 'xxup', 'xxrep', 'xxwrep']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.special_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/wiki/hi-2'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?TextLMDataBunch.from_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "textlm = TextLMDataBunch.from_folder(path=\"/home/jupyter/hindi2vec/data/\", max_vocab=100000, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>प्रक्रिया में सहायता और छूट और अच्छी तरह से किया जा रहा है को बढ़ावा देने के है । शब्द फ्रेंच मालिश सानना के घर्षण से आता है , या \" स्पर्श महसूस , या संभाल \" या लैटिन xxmaj xxunk से \" बड़े पैमाने पर , आटा \" अर्थ अरबी xxmaj xxunk अर्थ से cf . यूनानी क्रिया xxunk ( xxunk ) \" संभाल करने के लिए , स्पर्श</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>रहे पांचवीं भारतीय से जुड़ा उनका 11वां ब्रिगेड लौट आया था । उनका पांचवां ब्रिगेड जून 1942 में लौट आया और xxunk xxunk में युद्ध किया । मई से जून 1942 तक xxunk की लड़ाई में भाग लेने के समय सीरिया से दसवां भारतीय इन्फैन्ट्री डिवीजन पहुँच गया उसके बाद एल अलामीन की पहली लड़ाई में 72 घंटे तक एक्सिस सेना को रोके रखा जिससे आठवीं सेना को सुरक्षित ढंग</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>जाता है ( जैसे किसी दर्शक को दिखाया जाने वाला ऋण राहत से संबंधित कोई विज्ञापन जिसे मेल में देर से नोटिस प्राप्त हुआ हो ) या उसे बुनियादी सन्देश से परे कोई मनोरंजन प्राप्त होता है ( जैसे xxunk के \" xxunk द बीफ ? \" अभियान के लिए क्लासिक xxunk कार्यक्रम ) तो दर्शक उस विज्ञापन का इंतजार कर सकते हैं और शायद उसे फिर से देखने की</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>। क्या हम हाथ में है उनके प्रशंसकों द्वारा कलाकारों के ज्यादातर xxunk हैं । वैज्ञानिक मूल्यांकन और सौंदर्य व्याख्याओं दुर्लभ हैं । कला आलोचना एक xxunk में इसलिए है । इस स्थिति विश्वविद्यालय को बदलने के लिए प्रदर्शन कला के क्षेत्र में अनुसंधान और उच्च शिक्षा के लिए एक मजबूत नींव रखने पर ध्यान देना होगा । सांस्कृतिक अध्ययन , सांस्कृतिक पत्रकारिता , मल्टीमीडिया , मास कम्युनिकेशन , महिला</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>xxunk के कारण सार्वजनिक करने का आरोप xxunk पर लगाया । इस को अदालत के बाहर समझौता किया गया और रूनी ने उनसे माफी भी माँगा । xxunk सीज़न मे दोनो फिर से साथ हो गए । \\n  = हेनरी xxunk \\n  हेनरी फोंडा \\n  हेनरी xxunk फोंडा ( मई १६ , xxunk १२ , १९८२ ) एक अमेरिकी और रंगमंच अभिनेता थे । उन्होने ब्रॉडवे अभिनेता</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "textlm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "textlm.sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'valid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-eb60ac45f72c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     .add_test(TextList.from_df(test_df, model_path, cols=['texts'], processor=processor))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     .databunch(bs=32)))\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'valid'"
     ]
    }
   ],
   "source": [
    "data = (ItemLists(model_path,\n",
    "                  TextList.from_folder(path=\"data/\").no_split()\n",
    "#     TextList.from_df(train_df, model_path, cols=['texts'], processor=processor),\n",
    "#     TextList.from_df(valid_df, model_path, cols=['texts'], processor=processor))\n",
    "    .label_for_lm()\n",
    "#     .add_test(TextList.from_df(test_df, model_path, cols=['texts'], processor=processor))\n",
    "    .databunch(bs=32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = textlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "?textlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.58 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "filename = f\"{lang}_wiki_lm_data.pkl\"\n",
    "data.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.44 µs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-5c82fbeb536d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/jupyter/hindi2vec/data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "%time\n",
    "data = load_data(\"/home/jupyter/hindi2vec/data\")\n",
    "data.sanity_check()\n",
    "len(data.train_ds), len(data.valid_ds), len(data.test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>, हाथों से काम करने के लिए आटा xxunk भेद xxunk मालिश के लिए प्राचीन ग्रीक शब्द था और लैटिन xxunk था । \\n  मालिश पर अभिनय और दबाव के साथ शरीर से छेड़छाड़ शामिल है - संरचित , xxunk , स्थिर , या चलती - तनाव , गति , या कंपन किया मैन्युअल रूप से या यांत्रिक एड्स के साथ. टारगेट ऊतकों मांसपेशियों , xxunk , xxunk ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>से पीछे हटने का मौका मिला . \\n  एल अलामीन की दूसरी लड़ाई के लिए xxunk चौथा डिवीजन लौट आया और उसने आठवीं सेना की पंक्ति के केन्द्र में xxunk रिज पर कब्ज़ा किया और सीमा के केन्द्र की तरफ ध्यान xxunk के इरादे से एक नकली और दो छोटे हमले किए . \\n  ऑपरेशन xxunk ( चौथा भारतीय , दूसरा न्यूजीलैंड और xxunk xxunk डिवीजन ) xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>भी इच्छा रख सकते हैं । \\n  2 जनवरी 1971 से अमेरिकी टीवी होने वाले सिगरेट के विज्ञापनों पर प्रतिबन्ध लगा दिया गया । शराब उत्पादों के विज्ञापन की अनुमति है लेकिन टीवी विज्ञापन में किसी भी शराब उत्पाद के सेवन की अनुमति नहीं है । 1990 के दशक के अंतिम दौर से टीवी विज्ञापन ने अधिक विविध रूप धारण कर लिया है और xxunk घरेलू उत्पाद और खाद्य</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>अध्ययन में यह नई पीढ़ी के पाठ्यक्रमों के लिए , आदि प्रलेखन एक बहुत फार्म ऐसे पाठ्यक्रमों को फायदा होगा । \\n  विश्वविद्यालय के आदर्श वाक्य के नए युग की आत्मज्ञान की भावना जागृत पारंपरिक तरीके में कला शिक्षा के लिए एक मजबूत प्रणाली डिजाइन करने के xxunk आदर्श वाक्य के नए युग की आत्मज्ञान की भावना जागृत पारंपरिक तरीके में कला शिक्षा के लिए एक मजबूत प्रणाली डिजाइन</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>खूब नाम कमाया । १९३५ में उन्होने अपने हॉलीवुड करियर की शुरुआत की थी और अकादमी पुरस्कार के लिए नामांकित हुए , टॉम xxunk का पात्र ' ग्रेप्स ऑफ xxunk ' में निभाने के लिए । इस फिल्म के कारण उनके फिल्म करियर ने रफ्तार पकड़ ली और उसके बाद छह दशकों तक वे फिल्मो में काम करते रहे । उन्होने कई क्लासिक फिल्मो में काम किया , जैसे '</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[   12,  1510,    12,  ...,     8,    42,    11],\n",
       "         [   19,  2820,    33,  ...,     0,    69,     9],\n",
       "         [  333,    51,   659,  ...,   373,   120,    10],\n",
       "         ...,\n",
       "         [   13,   120,     9,  ...,    17,   409,  4186],\n",
       "         [ 1278,  8186,  5298,  ...,   195,  7691,    14],\n",
       "         [   31,  3492,    18,  ...,    12, 10915,    12]], device='cuda:0'),\n",
       " tensor([[ 1510,    12,  4324,  ...,    42,    11,   194],\n",
       "         [ 2820,    33,     9,  ...,    69,     9,    15],\n",
       "         [   51,   659,   111,  ...,   120,    10,   332],\n",
       "         ...,\n",
       "         [  120,     9,   738,  ...,   409,  4186, 18890],\n",
       "         [ 8186,  5298,    34,  ...,  7691,    14,    66],\n",
       "         [ 3492,    18,  2805,  ..., 10915,    12,  4161]], device='cuda:0')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data.vocab.itos, open(f'{model_path}/{lang}_wiki_itos.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_lm = data.vocab\n",
    "# vocab_lm.numericalize(word_tokenize('สวัสดีครับพี่น้อง', engine='ulmfit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/jupyter/stanfordnlp_resources/hi_hdtb_models/hi_hdtb_tokenizer.pt', 'lang': 'hi', 'shorthand': 'hi_hdtb', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16, 336, 756, 93, 88, 6619, 8, 1248, 21, 3999, 326, 64, 38]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_lm.numericalize(word_tokenize(\"से अमेरिकी टीवी होने वाले सिगरेट के विज्ञापनों पर प्रतिबन्ध लगा दिया गया\", lang=lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'सिगरेट के विज्ञापनों पर प्रतिबन्ध'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_lm.textify([6619, 8, 1248, 21, 3999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "\n",
    "We train the language model according to the [ULMFit paper](https://arxiv.org/abs/1801.06146). We use the name hyperparameters as [n-waves/ulmfit-multilingual](https://github.com/n-waves/ulmfit-multilingual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(emb_sz=400, n_hid=1550, n_layers=4, pad_token=1, qrnn=False, tie_weights=True, out_bias=True,\n",
    "             output_p=0.25, hidden_p=0.1, input_p=0.2, embed_p=0.02, weight_p=0.15)\n",
    "trn_args = dict(drop_mult=0.9, clip=0.12, alpha=2, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data, AWD_LSTM, config=config, pretrained=False, **trn_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "learn.callback_fns += [partial(CSVLogger, filename=\"logs\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 04:09 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.860805</td>\n",
       "      <td>6.651607</td>\n",
       "      <td>0.075194</td>\n",
       "      <td>04:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-1)\n",
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 54:59 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.199951</td>\n",
       "      <td>6.228636</td>\n",
       "      <td>0.133992</td>\n",
       "      <td>05:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.651200</td>\n",
       "      <td>5.698413</td>\n",
       "      <td>0.191571</td>\n",
       "      <td>05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.224508</td>\n",
       "      <td>5.287286</td>\n",
       "      <td>0.227300</td>\n",
       "      <td>05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.897160</td>\n",
       "      <td>5.019299</td>\n",
       "      <td>0.248006</td>\n",
       "      <td>05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.666602</td>\n",
       "      <td>4.864602</td>\n",
       "      <td>0.259487</td>\n",
       "      <td>05:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.482499</td>\n",
       "      <td>4.769929</td>\n",
       "      <td>0.266421</td>\n",
       "      <td>05:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.334930</td>\n",
       "      <td>4.724351</td>\n",
       "      <td>0.268507</td>\n",
       "      <td>05:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.223247</td>\n",
       "      <td>4.694628</td>\n",
       "      <td>0.270599</td>\n",
       "      <td>05:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.157586</td>\n",
       "      <td>4.681571</td>\n",
       "      <td>0.271860</td>\n",
       "      <td>05:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.182777</td>\n",
       "      <td>4.671778</td>\n",
       "      <td>0.272656</td>\n",
       "      <td>05:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 1e-3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 10:53 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.256379</td>\n",
       "      <td>4.698619</td>\n",
       "      <td>0.269978</td>\n",
       "      <td>05:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.055941</td>\n",
       "      <td>4.658815</td>\n",
       "      <td>0.272787</td>\n",
       "      <td>05:32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, 1e-3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 10:51 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.038492</td>\n",
       "      <td>4.683591</td>\n",
       "      <td>0.271404</td>\n",
       "      <td>05:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.935008</td>\n",
       "      <td>4.667615</td>\n",
       "      <td>0.271361</td>\n",
       "      <td>05:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, (1e-3)/2, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 1:00:17 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.295037</td>\n",
       "      <td>4.828024</td>\n",
       "      <td>0.263757</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.284737</td>\n",
       "      <td>4.840027</td>\n",
       "      <td>0.262369</td>\n",
       "      <td>06:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.240898</td>\n",
       "      <td>4.836672</td>\n",
       "      <td>0.262301</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.175188</td>\n",
       "      <td>4.839178</td>\n",
       "      <td>0.262597</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.086222</td>\n",
       "      <td>4.844824</td>\n",
       "      <td>0.261966</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.006917</td>\n",
       "      <td>4.866828</td>\n",
       "      <td>0.260113</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.941406</td>\n",
       "      <td>4.879977</td>\n",
       "      <td>0.259851</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.888287</td>\n",
       "      <td>4.888649</td>\n",
       "      <td>0.259220</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.876373</td>\n",
       "      <td>4.894383</td>\n",
       "      <td>0.259472</td>\n",
       "      <td>06:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.960637</td>\n",
       "      <td>4.877632</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>06:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 1e-3/3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'{lang}_wiki_lm')\n",
    "learn.save_encoder(f'{lang}_wiki_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eyeballing Test\n",
    "We perform eyeballing test by having the model \"fill in the blanks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import *\n",
    "from fastai import *    \n",
    "from fastai.text import * \n",
    "from fastai.callbacks import CSVLogger\n",
    "# data_path = f'{lang}-all-unk/'\n",
    "# model_path = f'{lang}wiki_data/'\n",
    "\n",
    "#data\n",
    "data = load_data(\"/home/jupyter/hindi2vec/data\")\n",
    "data.sanity_check()\n",
    "\n",
    "#lm\n",
    "config = dict(emb_sz=400, n_hid=1550, n_layers=4, pad_token=1, qrnn=False, tie_weights=True, out_bias=True,\n",
    "             output_p=0.25, hidden_p=0.1, input_p=0.2, embed_p=0.02, weight_p=0.15)\n",
    "trn_args = dict(drop_mult=0.9, clip=0.12, alpha=2, beta=1)\n",
    "\n",
    "learn = language_model_learner(data, AWD_LSTM, config=config, pretrained=False, **trn_args)\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "#load weights\n",
    "lang = \"hi\"\n",
    "learn.load(f'{lang}_wiki_lm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' इंडोनेशिया में आयी एक सूनामी  योजना पर एंग्री 1672 में उद्घाटन कर दिया । रेल'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict(' इंडोनेशिया में आयी एक सूनामी ',10, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "??learn.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "We extract the embedding layer of the encoder to be used in the same manner as `word2vec`. We can also create sentence vector by summing or averaging the vectors. For more details about `word2vec` use cases, see`word2vec_examples.ipynb`. Note that we use word vectors from `v0.1` since it was trained specifically for the purpose and has comparable dimensions to `fastText` embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T17:14:19.257675Z",
     "start_time": "2018-01-24T17:14:19.219043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45238, 400)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how we extracted the embeddings\n",
    "emb_weights = list(learn.model.named_parameters())[0][1]\n",
    "emb_np = to_np(emb_weights.data)\n",
    "emb_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(f'{model_path}models/thai2vec.vec',binary=False,\n",
    "                                         unicode_errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_word2vec_format(f'{model_path}models/thai2vec.vec',f'{model_path}models/thai2vec.vocab',False)\n",
    "model.save_word2vec_format(f'{model_path}models/thai2vec.bin',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get document vector from the language model by applying the encoder to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tt = ThaiTokenizer()\n",
    "def document_vector(ss, learn, data):\n",
    "    s = tt.tokenizer(ss)\n",
    "    t = torch.tensor(data.vocab.numericalize(s), requires_grad=False).to(device)\n",
    "    m = learn.model[0].encoder.to(device)\n",
    "    res = m(t).mean(0).cpu().detach().numpy()\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.066298,  0.307813,  0.246051,  0.008683, ..., -0.058363,  0.133258, -0.289954, -1.770246], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = 'วันนี้วันดีปีใหม่'\n",
    "document_vector(ss,learn,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.066298,  0.307813,  0.246051,  0.008683, ..., -0.058363,  0.133258, -0.289954, -1.770246], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pythainlp.ulmfit import *\n",
    "document_vector('วันนี้วันดีปีใหม่',learn,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
