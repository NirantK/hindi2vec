{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hindi2vec Language Model Pre-training\n",
    "\n",
    "The goal of this notebook is to train a language model using the [fast.ai](http://www.fast.ai/) version of [AWD LSTM Language Model](https://arxiv.org/abs/1708.02182), with data from [Hindi Wikipedia Dump](https://dumps.wikimedia.org/hiwiki/latest/hiwiki-latest-pages-articles.xml.bz2). \n",
    "\n",
    "## EDIT THIS \n",
    "# TK\n",
    "Using 40M/200k/200k tokens of train-validation-test split, we achieved validation perplexity of **27.81627 with 60,004 embeddings at 400 dimensions**, compared to state-of-the-art as of October 27, 2018 at **42.41 for English WikiText-2 by [Yang et al (2018)](https://arxiv.org/abs/1711.03953)**. To the best of our knowledge, there is no comparable research in Thai language at the point of writing (February 17, 2019).\n",
    "\n",
    "Our workflow is as follows:\n",
    "\n",
    "* Retrieve and process [Thai Wikipedia Dump](https://dumps.wikimedia.org/thwiki/latest/thwiki-latest-pages-articles.xml.bz2) according to [n-waves/ulmfit-multilingual](https://github.com/n-waves/ulmfit-multilingual)\n",
    "* Perform 40M/200k/200k tokens of train-validation-test split split\n",
    "* Minimal text cleaning and tokenization using `newmm` with frozen dictionary (`engine='ulmfit'`) of [pyThaiNLP](https://github.com/pyThaiNLP/pythainlp/)\n",
    "* Train language model\n",
    "* Evaluate model based on perplexity and eyeballing\n",
    "* Extract embeddings to use as \"word2vec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-25T03:30:25.407566Z",
     "start_time": "2018-01-25T03:30:21.597641Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai import *    \n",
    "from fastai.text import * \n",
    "from fastai.callbacks import CSVLogger\n",
    "\n",
    "lang = \"hi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Wikipedia dump, \n",
    "# Uncomment when running for the first time\n",
    "# url = f\"https://dumps.wikimedia.org/{lang}wiki/latest/{lang}wiki-latest-pages-articles.xml.bz2\"\n",
    "# !wget -c $url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the bz2 file\n",
    "filename = f\"{lang}wiki-latest-pages-articles.xml\"\n",
    "compressed_filename = filename+\".bz2\"\n",
    "# Uncomment when running for the first time\n",
    "# !bzip2 -dk $compressed_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when running for the first time\n",
    "# !git clone https://github.com/n-waves/ulmfit-multilingual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the dataset creation, pre- and post-processing of [n-waves/ulmfit-multilingual](https://github.com/n-waves/ulmfit-multilingual):\n",
    "\n",
    "* `ulmfit/create_wikitext.py` - Download thwiki in json format and separate them into 40M/200k/200k tokens of train-validation-test split. Articles with least than 100 tokens are removed. Also perform tokenization with whitespaces as separators.\n",
    "* `ulmfit/postprocess_wikitext.py` - Replace numbers and replace out-of-vocabulary tokens with `xxunk` (frequency of less than 3).\n",
    "\n",
    "We replaced the Moses Tokenizer with the following code to use [pyThaiNLP](https://github.com/pyThaiNLP/pythainlp/)'s `newmm` dictionary-based tokenizer with a frozen dictionary instead. We join the tokens within an article together to be tokenized later by the data bunch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when running for the first time\n",
    "# !git clone https://github.com/attardi/wikiextractor/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat wikiextractor/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikiextractor/WikiExtractor.py:2432: DeprecationWarning: Flags not at the start of the expression '\\\\[(((?i)bitcoin:|ftp' (truncated)\n",
      "  re.S | re.U)\n",
      "wikiextractor/WikiExtractor.py:2439: DeprecationWarning: Flags not at the start of the expression '^(http://|https://)(' (truncated)\n",
      "  re.X | re.S | re.U)\n",
      "wikiextractor/WikiExtractor.py:645: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  self.title, self.id, *errs)\n",
      "WARNING: Template errors in article 'अमीरात' (66023): title(1) recursion(0, 0, 0)\n",
      "wikiextractor/WikiExtractor.py:645: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  self.title, self.id, *errs)\n",
      "WARNING: Template errors in article 'पाकिस्तान की राष्ट्रीय सभा' (123118): title(1) recursion(0, 0, 0)\n",
      "WARNING: Template errors in article 'प्रथम आंग्ल-बर्मी युद्ध' (248956): title(1) recursion(0, 0, 0)\n",
      "wikiextractor/WikiExtractor.py:645: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  self.title, self.id, *errs)\n",
      "WARNING: Template errors in article 'मजलिस-ए-शूरा' (637466): title(1) recursion(0, 0, 0)\n",
      "WARNING: Template errors in article 'कृष्णगौड़ ब्राह्मण' (694111): title(1) recursion(0, 0, 0)\n",
      "WARNING: Template errors in article 'जोन बायज़' (812423): title(1) recursion(0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment when running for the first time, this can take a few minutes\n",
    "# This is run in quiet mode, remove --quiet if you want to see progress status\n",
    "\n",
    "# !python wikiextractor/WikiExtractor.py $filename -o data/wiki_extr --json --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/wiki_extr/AA:\r\n",
      "wiki_00  wiki_12  wiki_24  wiki_36  wiki_48  wiki_60  wiki_72  wiki_84\twiki_96\r\n",
      "wiki_01  wiki_13  wiki_25  wiki_37  wiki_49  wiki_61  wiki_73  wiki_85\twiki_97\r\n",
      "wiki_02  wiki_14  wiki_26  wiki_38  wiki_50  wiki_62  wiki_74  wiki_86\twiki_98\r\n",
      "wiki_03  wiki_15  wiki_27  wiki_39  wiki_51  wiki_63  wiki_75  wiki_87\twiki_99\r\n",
      "wiki_04  wiki_16  wiki_28  wiki_40  wiki_52  wiki_64  wiki_76  wiki_88\r\n",
      "wiki_05  wiki_17  wiki_29  wiki_41  wiki_53  wiki_65  wiki_77  wiki_89\r\n",
      "wiki_06  wiki_18  wiki_30  wiki_42  wiki_54  wiki_66  wiki_78  wiki_90\r\n",
      "wiki_07  wiki_19  wiki_31  wiki_43  wiki_55  wiki_67  wiki_79  wiki_91\r\n",
      "wiki_08  wiki_20  wiki_32  wiki_44  wiki_56  wiki_68  wiki_80  wiki_92\r\n",
      "wiki_09  wiki_21  wiki_33  wiki_45  wiki_57  wiki_69  wiki_81  wiki_93\r\n",
      "wiki_10  wiki_22  wiki_34  wiki_46  wiki_58  wiki_70  wiki_82  wiki_94\r\n",
      "wiki_11  wiki_23  wiki_35  wiki_47  wiki_59  wiki_71  wiki_83  wiki_95\r\n",
      "\r\n",
      "data/wiki_extr/AB:\r\n",
      "wiki_00  wiki_12  wiki_24  wiki_36  wiki_48  wiki_60  wiki_72  wiki_84\twiki_96\r\n",
      "wiki_01  wiki_13  wiki_25  wiki_37  wiki_49  wiki_61  wiki_73  wiki_85\twiki_97\r\n",
      "wiki_02  wiki_14  wiki_26  wiki_38  wiki_50  wiki_62  wiki_74  wiki_86\twiki_98\r\n",
      "wiki_03  wiki_15  wiki_27  wiki_39  wiki_51  wiki_63  wiki_75  wiki_87\twiki_99\r\n",
      "wiki_04  wiki_16  wiki_28  wiki_40  wiki_52  wiki_64  wiki_76  wiki_88\r\n",
      "wiki_05  wiki_17  wiki_29  wiki_41  wiki_53  wiki_65  wiki_77  wiki_89\r\n",
      "wiki_06  wiki_18  wiki_30  wiki_42  wiki_54  wiki_66  wiki_78  wiki_90\r\n",
      "wiki_07  wiki_19  wiki_31  wiki_43  wiki_55  wiki_67  wiki_79  wiki_91\r\n",
      "wiki_08  wiki_20  wiki_32  wiki_44  wiki_56  wiki_68  wiki_80  wiki_92\r\n",
      "wiki_09  wiki_21  wiki_33  wiki_45  wiki_57  wiki_69  wiki_81  wiki_93\r\n",
      "wiki_10  wiki_22  wiki_34  wiki_46  wiki_58  wiki_70  wiki_82  wiki_94\r\n",
      "wiki_11  wiki_23  wiki_35  wiki_47  wiki_59  wiki_71  wiki_83  wiki_95\r\n",
      "\r\n",
      "data/wiki_extr/AC:\r\n",
      "wiki_00  wiki_12  wiki_24  wiki_36  wiki_48  wiki_60  wiki_72  wiki_84\twiki_96\r\n",
      "wiki_01  wiki_13  wiki_25  wiki_37  wiki_49  wiki_61  wiki_73  wiki_85\twiki_97\r\n",
      "wiki_02  wiki_14  wiki_26  wiki_38  wiki_50  wiki_62  wiki_74  wiki_86\twiki_98\r\n",
      "wiki_03  wiki_15  wiki_27  wiki_39  wiki_51  wiki_63  wiki_75  wiki_87\twiki_99\r\n",
      "wiki_04  wiki_16  wiki_28  wiki_40  wiki_52  wiki_64  wiki_76  wiki_88\r\n",
      "wiki_05  wiki_17  wiki_29  wiki_41  wiki_53  wiki_65  wiki_77  wiki_89\r\n",
      "wiki_06  wiki_18  wiki_30  wiki_42  wiki_54  wiki_66  wiki_78  wiki_90\r\n",
      "wiki_07  wiki_19  wiki_31  wiki_43  wiki_55  wiki_67  wiki_79  wiki_91\r\n",
      "wiki_08  wiki_20  wiki_32  wiki_44  wiki_56  wiki_68  wiki_80  wiki_92\r\n",
      "wiki_09  wiki_21  wiki_33  wiki_45  wiki_57  wiki_69  wiki_81  wiki_93\r\n",
      "wiki_10  wiki_22  wiki_34  wiki_46  wiki_58  wiki_70  wiki_82  wiki_94\r\n",
      "wiki_11  wiki_23  wiki_35  wiki_47  wiki_59  wiki_71  wiki_83  wiki_95\r\n",
      "\r\n",
      "data/wiki_extr/AD:\r\n",
      "wiki_00  wiki_11  wiki_22  wiki_33  wiki_44  wiki_55  wiki_66  wiki_77\twiki_88\r\n",
      "wiki_01  wiki_12  wiki_23  wiki_34  wiki_45  wiki_56  wiki_67  wiki_78\twiki_89\r\n",
      "wiki_02  wiki_13  wiki_24  wiki_35  wiki_46  wiki_57  wiki_68  wiki_79\twiki_90\r\n",
      "wiki_03  wiki_14  wiki_25  wiki_36  wiki_47  wiki_58  wiki_69  wiki_80\r\n",
      "wiki_04  wiki_15  wiki_26  wiki_37  wiki_48  wiki_59  wiki_70  wiki_81\r\n",
      "wiki_05  wiki_16  wiki_27  wiki_38  wiki_49  wiki_60  wiki_71  wiki_82\r\n",
      "wiki_06  wiki_17  wiki_28  wiki_39  wiki_50  wiki_61  wiki_72  wiki_83\r\n",
      "wiki_07  wiki_18  wiki_29  wiki_40  wiki_51  wiki_62  wiki_73  wiki_84\r\n",
      "wiki_08  wiki_19  wiki_30  wiki_41  wiki_52  wiki_63  wiki_74  wiki_85\r\n",
      "wiki_09  wiki_20  wiki_31  wiki_42  wiki_53  wiki_64  wiki_75  wiki_86\r\n",
      "wiki_10  wiki_21  wiki_32  wiki_43  wiki_54  wiki_65  wiki_76  wiki_87\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/wiki_extr/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to data/wiki/hi-2/hi.wiki.train.tokens...\n",
      "Processed 10,000 documents. Total # tokens: 1,912,375.\n",
      "data/wiki/hi-2/hi.wiki.train.tokens. # documents: 11,947. # tokens: 2,000,063.\n",
      "Copying data/wiki/hi-2/hi.wiki.train.tokens to data/wiki/hi-100/hi.wiki.train.tokens & data/wiki/hi-all/hi.wiki.train.tokens.\n",
      "Writing to data/wiki/hi-2/hi.wiki.valid.tokens...\n",
      "data/wiki/hi-2/hi.wiki.valid.tokens. # documents: 242. # tokens: 200,986.\n",
      "Copying data/wiki/hi-2/hi.wiki.valid.tokens to data/wiki/hi-100/hi.wiki.valid.tokens & data/wiki/hi-all/hi.wiki.valid.tokens.\n",
      "Writing to data/wiki/hi-2/hi.wiki.test.tokens...\n",
      "data/wiki/hi-2/hi.wiki.test.tokens. # documents: 154. # tokens: 204,765.\n",
      "Copying data/wiki/hi-2/hi.wiki.test.tokens to data/wiki/hi-100/hi.wiki.test.tokens & data/wiki/hi-all/hi.wiki.test.tokens.\n",
      "Writing to data/wiki/hi-100/hi.wiki.train.tokens...\n",
      "Processed 10,000 documents. Total # tokens: 4,134,864.\n",
      "Processed 30,000 documents. Total # tokens: 11,013,897.\n",
      "Processed 40,000 documents. Total # tokens: 15,473,725.\n",
      "Processed 50,000 documents. Total # tokens: 20,767,058.\n",
      "Processed 70,000 documents. Total # tokens: 30,358,224.\n",
      "Processed 80,000 documents. Total # tokens: 33,984,566.\n",
      "Processed 110,000 documents. Total # tokens: 41,708,763.\n",
      "data/wiki/hi-100/hi.wiki.train.tokens. # documents: 112,225. # tokens: 42,471,286.\n",
      "Writing to data/wiki/hi-all/hi.wiki.train.tokens...\n",
      "data/wiki/hi-all/hi.wiki.train.tokens. # documents: 0. # tokens: 0.\n"
     ]
    }
   ],
   "source": [
    "!python ulmfit-multilingual/ulmfit/create_wikitext.py -i data/wiki_extr/ -o data/wiki/ -l hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4364 data/wiki/hi-2/hi.wiki.valid.tokens\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/wiki/hi-2/hi.wiki.valid.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"data/wiki/{lang}-2/\"\n",
    "model_path = f\"models/wiki/{lang}-2\"\n",
    "assert Path(data_path).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def istitle(line):\n",
    "    return len(re.findall(r'^ ?=\\|[^=]*\\|= ?$', line)) != 0\n",
    "\n",
    "def read_wiki_articles(filename):\n",
    "    articles = []\n",
    "    with open(filename, encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "    current_article = []\n",
    "    for i,line in enumerate(lines):\n",
    "        current_article.append(line)\n",
    "        if i < len(lines)-2 and lines[i+1].strip() == \"\" and istitle(lines[i+2]):\n",
    "            articles.append(\"\".join(current_article))\n",
    "            current_article = []\n",
    "        articles.append(\"\".join(current_article))\n",
    "#     print(current_article[:10])\n",
    "    print(f\"Wiki text was split to {len(articles)} articles\")\n",
    "    return pd.DataFrame({'texts': np.array(articles, dtype=np.object)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last line is corrupted\n",
    "train_df = read_wiki_articles(f'{data_path}{lang}.wiki.train.tokens').iloc[:-1,:]\n",
    "valid_df = read_wiki_articles(f'{data_path}{lang}.wiki.valid.tokens').iloc[:-1,:]\n",
    "test_df = read_wiki_articles(f'{data_path}{lang}.wiki.test.tokens').iloc[:-1,:]\n",
    "train_df.shape, valid_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [texts]\n",
       "Index: []"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join tokens together, I think this is unique to Thai tokenization\n",
    "train_df['texts'] = train_df.texts.map(lambda x: ''.join(x.split('|')))\n",
    "valid_df['texts'] = valid_df.texts.map(lambda x: ''.join(x.split('|')))\n",
    "test_df['texts'] = test_df.texts.map(lambda x: ''.join(x.split('|')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: texts, dtype: object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['texts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We used the `newmm` engine of `pyThaiNLP` to perform tokenization. Out of randomnum tokens from all of training set, we chose 60,000 embeddings (plus two for unknown and padding) of tokens which appeared more than twice (not typos) in the training set.\n",
    "\n",
    "\n",
    "We perform the following text processing:\n",
    "\n",
    "* Fix html tags to plain texts\n",
    "* Lowercase all English words and if a word is written in all caps, we put it in a lower case and add `xxup` before\n",
    "* Repetitive characters: Thai usually emphasizes adjectives by repeating the last character such as `อร่อยมากกกกกกก` to `อร่อยมาก xxrep 7 ` so that the word still retains its original form. \n",
    "* Normalize character order: for instance `นำ้` to `น้ำ`\n",
    "* Add spaces around / and #\n",
    "* Remove multiple spaces and newlines\n",
    "* Remove empty brackets of all types (`([{`) which might result from cleaning up\n",
    "* `pyThaiNLP`'s `newmm` word tokenizer with frozen dictionary (`engine ='ulmfit'`)  is used to tokenize the texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thai Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `newmm` tokenizer with a dictionary frozen as of 2018-10-23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T15:50:02.035245Z",
     "start_time": "2018-01-24T15:50:01.940191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['วิทยาศาสตร์',\n",
       " 'ดาวเคราะห์',\n",
       " 'เป็น',\n",
       " 'สาขาวิชา',\n",
       " 'ที่',\n",
       " 'ศึกษา',\n",
       " 'เกี่ยวกับ',\n",
       " 'องค์ประกอบ',\n",
       " 'ของ',\n",
       " 'ดาวเคราะห์']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='วิทยาศาสตร์ดาวเคราะห์เป็นสาขาวิชาที่ศึกษาเกี่ยวกับองค์ประกอบของดาวเคราะห์'\n",
    "a = word_tokenize(text,engine='ulmfit')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#integrated into pythainlp.ulmfit.utils\n",
    "from fastai.text.transform import *\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.util import normalize as normalize_char_order\n",
    "\n",
    "class ThaiTokenizer(BaseTokenizer):\n",
    "    \"Wrapper around a newmm tokenizer to make it a `BaseTokenizer`.\"\n",
    "    def __init__(self, lang = 'th'):\n",
    "        self.lang = lang\n",
    "    def tokenizer(self, t):\n",
    "        return word_tokenize(t,engine='ulmfit')\n",
    "    def add_special_cases(self, toks):\n",
    "        pass\n",
    "    \n",
    "def replace_rep_after(t):\n",
    "    \"Replace repetitions at the character level in `t` after the repetition\"\n",
    "    def _replace_rep(m):\n",
    "        c,cc = m.groups()\n",
    "        return f'{c} {TK_REP} {len(cc)+1} '\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return re_rep.sub(_replace_rep, t)\n",
    "\n",
    "def rm_useless_newlines(t):\n",
    "    \"Remove multiple newlines in `t`.\"\n",
    "    return re.sub('[\\n]{2,}', ' ', t)\n",
    "\n",
    "def rm_brackets(t):\n",
    "    \"Remove all empty brackets from `t`.\"\n",
    "    new_line = re.sub('\\(\\)','',t)\n",
    "    new_line = re.sub('\\{\\}','',new_line)\n",
    "    new_line = re.sub('\\[\\]','',new_line)\n",
    "    return(new_line)\n",
    "\n",
    "#in case we want to add more specific rules for thai\n",
    "pre_rules_th = [fix_html, replace_rep_after, normalize_char_order, \n",
    "                spec_add_spaces, rm_useless_spaces, rm_useless_newlines, rm_brackets]\n",
    "post_rules_th = [replace_all_caps, deal_caps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Bunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained the language model based on 40M/200k/200k tokens of train-validation-test split from [Thai Wikipedia Dump](https://dumps.wikimedia.org/thwiki/latest/thwiki-latest-pages-articles.xml.bz2). Tokens are generated and numericalized filtering all words with frequency more than 3 and at maximum vocab size of 60,000 (plus unknown and padding tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = Tokenizer(tok_func = ThaiTokenizer, lang = 'th', pre_rules = pre_rules_th, post_rules=post_rules_th)\n",
    "processor = [TokenizeProcessor(tokenizer=tt, chunksize=10000, mark_fields=False),\n",
    "            NumericalizeProcessor(vocab=None, max_vocab=60000, min_freq=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (ItemLists(model_path, \n",
    "    TextList.from_df(train_df, model_path, cols=['texts'], processor=processor),\n",
    "    TextList.from_df(valid_df, model_path, cols=['texts'], processor=processor))\n",
    "    .label_for_lm()\n",
    "    .add_test(TextList.from_df(test_df, model_path, cols=['texts'], processor=processor))\n",
    "    .databunch(bs=64))\n",
    "data.sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "data.save(f\"{lang}wiki_lm_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20496, 491, 476)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "data = load_data(model_path,\"{lang}wiki_lm_data.pkl\")\n",
    "data.sanity_check()\n",
    "len(data.train_ds), len(data.valid_ds), len(data.test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ที่ บ้าน คุณตา คุณยาย   ขณะที่ หยุดพัก ใต้ ร่มไม้ ใหญ่ ชายป่า   เกิด เจ็บท้อง กะทันหัน   และ ได้ คลอด บุตรชาย ออกมา   จึง ได้ ตั้ง ชื่อว่า   “ เด็กชาย พัก ”   \\n   \\n   บรรพชา และ อุปสมบท .   \\n   เมื่อ อายุ   ๘   ขวบ   โยม บิดา ได้ พา เข้า กรุง เทพ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ถ่ายทำ พร้อมกับ เรื่อง   \"อ เวน เจอ ร์ส:   มหา สงคราม ล้าง จักรวาล \"   และ ถ่ายทำ เสร็จสิ้น ใน เดือน มกราคม   ค.ศ.   2018   ชื่อเรื่อง   xxmaj avengers :   xxmaj xxunk   เปิดเผย อย่างเป็นทางการ ใน ตัวอย่าง ภาพยนตร์ ที่ เผยแพร่ ใน เดือน ธันวาคม   2018   \\n   \\n   อ เวน เจอ ร์ส:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>\\n   \\n   พลัง ศักติ ของ พระองค์ :   ผู้ ที่ นับถือ บูชา เทวี องค์ นี้ จะ ได้รับ พร จาก พระ ส กันทะ   ( หรือ ที่ เรียก ว่า   กร รติ เก ยะ )   เทพ แห่ง สงคราม   \\n   \\n   ประวัติ ความเป็นมา ของ พระองค์ :   พระองค์ ถือกำเนิด มา เพื่อ มา ปราบ อสูร ที่ มี</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>นิ้ว )   ใกล้ กับ แหลม   xxmaj xxunk   และ กองทัพเรือ ฟินแลนด์ ได้ รวม รวม กองเรือ ตอร์ปิโด มอเตอร์   ( xxmaj motor   xxmaj xxunk   xxmaj boat )   ที่   2   กับ เรือ xxunk   xxup xxunk 9,   xxup xxunk 10,   xxup xxunk 11   และ   xxup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>แสดง ทัศนะ ด้าน การเมือง   ศาสนา   หรือ คำคม ส่วนตัว   โดย พิมพ์ ไว้ บน เสื้อ ใน   เป็นเหตุให้ คณะกรรมการ สมาคม ฟุตบอล ระหว่างประเทศ ออก กฎ ใน ปี   ค.ศ.   2002   ห้าม มี การเขียน หรือ โล โก้ ใด บน เสื้อ ใน   จนกระทั่ง ใน ปี   ค.ศ.   2004   ได้ เพิ่ม กฎ ห้าม ผู้ เล่น ถอด เสื้อ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[    2,     8,    63,  ...,    76,  3053,     8],\n",
       "         [  211,  3310,   786,  ...,    19,   426,     8],\n",
       "         [14105,    41,     8,  ...,     9,     8,     9],\n",
       "         ...,\n",
       "         [   62,     8,   468,  ...,    29,  1854,    74],\n",
       "         [   23,     8,     4,  ...,     4,  5479,     8],\n",
       "         [   12,     0,   302,  ...,   302,  8087,    21]], device='cuda:0'),\n",
       " tensor([[    8,    63, 11912,  ...,  3053,     8,   387],\n",
       "         [ 3310,   786,     8,  ...,   426,     8,  1995],\n",
       "         [   41,     8,   328,  ...,     8,     9,     8],\n",
       "         ...,\n",
       "         [    8,   468,     8,  ...,  1854,    74,  1854],\n",
       "         [    8,     4, 12691,  ...,  5479,     8,    21],\n",
       "         [    0,   302,    98,  ...,  8087,    21,    17]], device='cuda:0')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data.test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data.vocab.itos, open(f'{model_path}models/thwiki_itos.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10211, 7232, 1363]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_lm = data.vocab\n",
    "vocab_lm.numericalize(word_tokenize('สวัสดีครับพี่น้อง', engine='ulmfit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'สวัสดี ครับ พี่น้อง'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_lm.textify([10211, 7232, 1363])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling\n",
    "\n",
    "We train the language model according to the [ULMFit paper](https://arxiv.org/abs/1801.06146). We use the name hyperparameters as [n-waves/ulmfit-multilingual](https://github.com/n-waves/ulmfit-multilingual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(emb_sz=400, n_hid=1550, n_layers=4, pad_token=1, qrnn=False, tie_weights=True, out_bias=True,\n",
    "             output_p=0.25, hidden_p=0.1, input_p=0.2, embed_p=0.02, weight_p=0.15)\n",
    "trn_args = dict(drop_mult=0.9, clip=0.12, alpha=2, beta=1)\n",
    "\n",
    "learn = language_model_learner(data, AWD_LSTM, config=config, pretrained=False, **trn_args)\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "learn.callback_fns += [partial(CSVLogger, filename=\"logs\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 2.09E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.freeze_to(-1)\n",
    "# learn.fit_one_cycle(1, 1e-2, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.unfreeze()\n",
    "# learn.fit_one_cycle(10, 1e-3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| epoch | train_loss | valid_loss | accuracy |\n",
    "|-------|------------|------------|----------|\n",
    "| 1     | 5.689881   | 5.479321   | 0.176962 |\n",
    "| 2     | 5.14165    | 4.914922   | 0.223256 |\n",
    "| 3     | 4.945699   | 4.661891   | 0.242748 |\n",
    "| 4     | 4.708307   | 4.471861   | 0.261348 |\n",
    "| 5     | 4.636846   | 4.337728   | 0.273785 |\n",
    "| 6     | 4.487616   | 4.237517   | 0.285252 |\n",
    "| 7     | 4.370384   | 4.161622   | 0.293807 |\n",
    "| 8     | 4.291326   | 4.100852   | 0.30109  |\n",
    "| 9     | 4.267814   | 4.071167   | 0.303888 |\n",
    "| 10    | 4.217663   | 4.065733   | 0.304822 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.unfreeze()\n",
    "# learn.fit_one_cycle(10, 1e-3/3, moms=(0.8, 0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| epoch | train_loss | valid_loss | accuracy |\n",
    "|-------|------------|------------|----------|\n",
    "| 1     | 4.179729   | 4.052608   | 0.304963 |\n",
    "| 2     | 4.16306    | 4.047963   | 0.305331 |\n",
    "| 3     | 4.184966   | 4.044866   | 0.305482 |\n",
    "| 4     | 4.218517   | 4.023499   | 0.307901 |\n",
    "| 5     | 4.183966   | 4.001536   | 0.310346 |\n",
    "| 6     | 4.165901   | 3.985509   | 0.312516 |\n",
    "| 7     | 4.091527   | 3.966779   | 0.315573 |\n",
    "| 8     | 4.078772   | 3.952682   | 0.317021 |\n",
    "| 9     | 4.020618   | 3.944576   | 0.318096 |\n",
    "| 10    | 4.043382   | 3.94425    | 0.317961 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.save('thwiki_lm')\n",
    "# learn.save_encoder('thwiki_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eyeballing Test\n",
    "We perform eyeballing test by having the model \"fill in the blanks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from fastai import *    \n",
    "from fastai.text import * \n",
    "from fastai.callbacks import CSVLogger\n",
    "data_path = 'th-all-unk/'\n",
    "model_path = 'thwiki_data/'\n",
    "\n",
    "#data\n",
    "data = load_data(model_path,'thwiki_lm_data.pkl')\n",
    "data.sanity_check()\n",
    "\n",
    "#lm\n",
    "config = dict(emb_sz=400, n_hid=1550, n_layers=4, pad_token=1, qrnn=False, tie_weights=True, out_bias=True,\n",
    "             output_p=0.25, hidden_p=0.1, input_p=0.2, embed_p=0.02, weight_p=0.15)\n",
    "trn_args = dict(drop_mult=0.9, clip=0.12, alpha=2, beta=1)\n",
    "\n",
    "learn = language_model_learner(data, AWD_LSTM, config=config, pretrained=False, **trn_args)\n",
    "learn.opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "#load weights\n",
    "learn.load('thwiki_lm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'สวัสดีครับ พี่น้องเสื้อแดง=รองเท้าแดงเสื้อแดง(อังกฤษ:St.โก้)เป็นภาพยนตร์ไทยแนวตลก-แฟนตาซีออกฉายในปีพ.ศ.2544กำกับโดยยุทธนามุกดาสนิทนำแสดงโดยสันติสุขพรหมศิริ,สุวนันท์คงยิ่ง,สุรชัยจันทิมาธร,บุญชูสกุลเจริญสุข,สุกัญญาวงศ์สวัสดิ์,สุกัญญาไชยศิริ,วิยะดาอุมารินทร์,นิรุตติ์ศิริจรรยา,อุบลรัตน์ปิยะศิริ,สุรชัยจันทิมาธร,รัช'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict('สวัสดีครับ พี่น้องเสื้อ',100, sep='', temperature = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "We extract the embedding layer of the encoder to be used in the same manner as `word2vec`. We can also create sentence vector by summing or averaging the vectors. For more details about `word2vec` use cases, see`word2vec_examples.ipynb`. Note that we use word vectors from `v0.1` since it was trained specifically for the purpose and has comparable dimensions to `fastText` embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-24T17:14:19.257675Z",
     "start_time": "2018-01-24T17:14:19.219043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60003, 400)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how we extracted the embeddings\n",
    "emb_weights = list(learn.model.named_parameters())[0][1]\n",
    "emb_np = to_np(emb_weights.data)\n",
    "emb_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(f'{model_path}models/thai2vec.vec',binary=False,\n",
    "                                         unicode_errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_word2vec_format(f'{model_path}models/thai2vec.vec',f'{model_path}models/thai2vec.vocab',False)\n",
    "model.save_word2vec_format(f'{model_path}models/thai2vec.bin',None,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get document vector from the language model by applying the encoder to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tt = ThaiTokenizer()\n",
    "def document_vector(ss, learn, data):\n",
    "    s = tt.tokenizer(ss)\n",
    "    t = torch.tensor(data.vocab.numericalize(s), requires_grad=False).to(device)\n",
    "    m = learn.model[0].encoder.to(device)\n",
    "    res = m(t).mean(0).cpu().detach().numpy()\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.066298,  0.307813,  0.246051,  0.008683, ..., -0.058363,  0.133258, -0.289954, -1.770246], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = 'วันนี้วันดีปีใหม่'\n",
    "document_vector(ss,learn,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.066298,  0.307813,  0.246051,  0.008683, ..., -0.058363,  0.133258, -0.289954, -1.770246], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pythainlp.ulmfit import *\n",
    "document_vector('วันนี้วันดีปีใหม่',learn,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
